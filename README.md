# Building Production Ready Machine Learning System

This project demonstrates production-ready machine learning pipelines with comprehensive MLflow artifact tracking, focusing on customer churn prediction.

## ğŸ¯ Project Overview

A complete ML system with enhanced MLflow tracking that provides:
- **Comprehensive Data Lineage**
- **Rich Artifact Management**
- **Production-Ready Monitoring**
- **Complete Reproducibility**

This repository implements a **production-ready machine learning pipeline** for customer churn prediction with comprehensive MLOps practices. The project demonstrates enterprise-grade ML engineering with:

- **Dual Implementation**: Scikit-learn (standard) and PySpark (distributed) pipelines
- **Orchestration**: Apache Airflow for workflow management
- **Experiment Tracking**: MLflow for versioning, metrics, and model registry
- **Observability**: Enhanced logging, monitoring, and data lineage tracking
- **Scalability**: Distributed processing with PySpark for big data scenarios
- **Reproducibility**: Version-controlled experiments and automated pipelines

---

## ğŸ“‚ Repository Structure

```
production-ready-ml-pipeline/
â”œâ”€â”€ README.md                          # Project documentation
â”œâ”€â”€ Makefile                           # Automation commands
â”œâ”€â”€ config.yaml                        # Configuration parameters
â”œâ”€â”€ requirements.txt                   # Python dependencies
â”‚
â”œâ”€â”€ .airflow/                          # Airflow home directory
â”‚   â”œâ”€â”€ airflow.cfg                    # Airflow configuration
â”‚   â”œâ”€â”€ airflow.db                     # Metadata database
â”‚   â”œâ”€â”€ dags/                          # DAG definitions
â”‚   â”‚   â”œâ”€â”€ data_pipeline_dag.py
â”‚   â”‚   â””â”€â”€ train_pipeline_dag.py
â”‚   â””â”€â”€ logs/                          # Airflow execution logs
â”‚
â”œâ”€â”€ artifacts/                         # Training artifacts
â”‚   â”œâ”€â”€ data/                          # Processed datasets
â”‚   â”‚   â”œâ”€â”€ X_train.csv
â”‚   â”‚   â”œâ”€â”€ X_test.csv
â”‚   â”‚   â”œâ”€â”€ Y_train.csv
â”‚   â”‚   â””â”€â”€ Y_test.csv
â”‚   â”œâ”€â”€ encode/                        # Feature encoders
â”‚   â”‚   â”œâ”€â”€ Gender_encoder.json
â”‚   â”‚   â””â”€â”€ Geography_encoder.json
â”‚   â”œâ”€â”€ models/                        # Trained models
â”‚   â”‚   â””â”€â”€ churn_analysis.joblib
â”‚   â””â”€â”€ mlflow_run_artifacts/          # MLflow artifacts by run
â”‚       â””â”€â”€ {run_id}/
â”‚           â”œâ”€â”€ visualizations/
â”‚           â””â”€â”€ final_csv_files/
â”‚
â”œâ”€â”€ data/                              # Data storage
â”‚   â”œâ”€â”€ raw/                           # Original datasets
â”‚   â”‚   â””â”€â”€ ChurnModelling.csv
â”‚   â””â”€â”€ processed/                     # Cleaned datasets
â”‚       â””â”€â”€ imputed.csv
â”‚
â”œâ”€â”€ mlruns/                            # MLflow tracking store
â”‚   â”œâ”€â”€ 0/                             # Default experiment
â”‚   â”œâ”€â”€ models/                        # Model registry
â”‚   â””â”€â”€ {experiment_id}/               # Experiment runs
â”‚       â””â”€â”€ {run_id}/
â”‚           â”œâ”€â”€ artifacts/
â”‚           â”œâ”€â”€ metrics/
â”‚           â”œâ”€â”€ params/
â”‚           â””â”€â”€ tags/
â”‚
â”œâ”€â”€ pipelines/                         # Pipeline orchestration
â”‚   â”œâ”€â”€ data_pipeline.py               # Data preprocessing
â”‚   â”œâ”€â”€ training_pipeline.py           # Model training
â”‚   â””â”€â”€ streaming_inference_pipeline.py # Batch inference
â”‚
â”œâ”€â”€ src/                               # Core modules
â”‚   â”œâ”€â”€ data_ingestion.py              # Data loading
â”‚   â”œâ”€â”€ data_splitter.py               # Train/test split
â”‚   â”œâ”€â”€ feature_binning.py             # Feature discretization
â”‚   â”œâ”€â”€ feature_encoding.py            # Categorical encoding
â”‚   â”œâ”€â”€ feature_scaling.py             # Normalization
â”‚   â”œâ”€â”€ handle_missing_values.py       # Imputation
â”‚   â”œâ”€â”€ model_building.py              # Model architecture
â”‚   â”œâ”€â”€ model_evaluation.py            # Performance metrics
â”‚   â”œâ”€â”€ model_inference.py             # Predictions
â”‚   â”œâ”€â”€ model_training.py              # Training logic
â”‚   â””â”€â”€ outlier_detection.py           # Anomaly detection
â”‚
â””â”€â”€ utils/                             # Helper functions
    â”œâ”€â”€ airflow_tasks.py               # Airflow task definitions
    â”œâ”€â”€ config.py                      # Config management
    â””â”€â”€ mlflow_utils.py                # MLflow helpers
```

---

## âœ¨ Key Features

### 1. ğŸ“Š Enhanced Data Pipeline
- **Stage-wise Data Profiling**: Track data quality at each transformation step
  - Raw data â†’ Missing value handling â†’ Encoding â†’ Scaling â†’ Final
- **Automatic Visualizations**: 
  - Feature distributions (histograms, box plots)
  - Correlation heatmaps
  - Missing value patterns
- **Data Lineage Tracking**: Full traceability using MLflow datasets
- **Quality Metrics**: 
  - Row/column counts
  - Missing value percentages
  - Memory usage
  - Data drift detection
- **Error Handling**: Comprehensive logging and failure recovery

### 2. ğŸ“ Enhanced Training Pipeline
- **Model Performance Tracking**:
  - Confusion matrices
  - Accuracy
  - Precision 
  - Recall 
  - F1-Score
- **Training Metrics**:
  - Training time
  - Model size
  - Hyperparameters
  - Cross-validation scores
- **Model Registry**: Versioned models with metadata
- **MLflow Integration**: Full experiment reproducibility

### 3. ğŸ”® Enhanced Inference Pipeline
- **Batch Prediction Tracking**: Monitor inference jobs
- **Performance Monitoring**:
  - Inference time per batch
  - Prediction distribution
  - Confidence scores
- **Logging**: Predictions and metrics logged to MLflow
- **Model Serving**: Ready for deployment integration

### 4. ğŸ”¬ MLflow Integration
- **Experiment Tracking**: Parameters, metrics, artifacts
- **Model Versioning**: Automatic model registry
- **Dataset Tracking**: Input/output data lineage
- **Artifact Management**: Organized storage by run ID
- **Visualization**: Interactive plots and dashboards

---

## ğŸ› ï¸ Prerequisites

### System Requirements
- **Python**: â‰¥ 3.10
- **Java**: â‰¥ 8 (required for PySpark)
- **Memory**: â‰¥ 8GB RAM recommended
- **Storage**: â‰¥ 5GB free space

### Required Tools
```bash
python3 --version  # Python 3.10+
java -version      # Java 8+
pip --version      # Latest pip
git --version      # Git for version control
```

### Python Packages
- **ML/Data**: scikit-learn, pandas, numpy
- **Big Data**: pyspark â‰¥ 3.x
- **MLOps**: mlflow â‰¥ 2.x
- **Orchestration**: apache-airflow â‰¥ 2.x
- **Visualization**: matplotlib, seaborn

---

## ğŸš€ Quick Start

### 1. Clone the Repository
```bash
git clone https://github.com/madhurabe00/production-ready-ml-pipeline.git
cd "Production ready pipeline/pyspark"
```

### 2. Set Up Environment

#### Linux/WSL (Recommended for Airflow)
```bash
# Create virtual environment
python3 -m venv .venv
source .venv/bin/activate

# Install dependencies
pip install --upgrade pip
pip install -r requirements.txt
```

#### Windows (Standard Pipeline Only)
```powershell
# Create virtual environment
python -m venv .venv
.\.venv\Scripts\Activate.ps1

# Install dependencies
pip install --upgrade pip
pip install -r requirements.txt
```

### 3. Initialize Airflow (Optional - for PySpark pipelines)
```bash
# Set Airflow home directory
export AIRFLOW_HOME=$(pwd)/.airflow

# Initialize database
airflow db init

# Create admin user
airflow users create \
    --username admin \
    --password admin \
    --firstname Admin \
    --lastname User \
    --role Admin \
    --email admin@example.com

# Create DAGs folder
mkdir -p .airflow/dags
```

### 4. Configure the Project
Edit `config.yaml` to customize:
```yaml
data:
  raw_path: "data/raw/churndataset.csv"
  processed_path: "data/processed/"

model:
  name: "RandomForestClassifier"
  hyperparameters:
    n_estimators: 100
    max_depth: 10

mlflow:
  tracking_uri: "file:./mlruns"
  experiment_name: "churn_prediction"
```

---

## ğŸ® Running the Pipelines

### Option 1: Using Makefile (Recommended)

```bash
# Run data pipeline
make data-pipeline

# Train model
make train-pipeline

# Run inference
make streaming-inference

# Start MLflow UI
make mlflow-ui

# Start Airflow (if configured)
make airflow-start

# Run all pipelines in sequence
run-all

# Stop all running MLflow servers
stop-all
```

### Option 2: Direct Execution

#### Data Pipeline
```bash
# Activate environment
source .venv/bin/activate  # Linux/WSL
# OR
.\.venv\Scripts\Activate.ps1  # Windows

# Change directory to scikit implemetation or spark implementation

cd ./scikit 
# OR
cd ./pyspark

# Run data pipeline
python pipelines/data_pipeline.py
```

#### Training Pipeline
```bash
python pipelines/training_pipeline.py
```

#### Inference Pipeline
```bash
python pipelines/streaming_inference_pipeline.py
```

### Option 3: Using Airflow (Distributed Processing)

```bash
# Start Airflow webserver (Terminal 1)
export AIRFLOW_HOME=$(pwd)/.airflow
airflow webserver -p 8080

# Start Airflow scheduler (Terminal 2)
export AIRFLOW_HOME=$(pwd)/.airflow
airflow scheduler

# OR
airflow standalone

# Access Airflow UI
# URL: http://localhost:8080
# Username: admin
# Password: admin

# Enable and trigger DAGs through the UI
```

---

## ğŸ“Š Monitoring and Visualization

### MLflow Tracking UI
```bash
# Start MLflow UI
mlflow ui --port 5000

# Access at: http://localhost:5000
```

**Features**:
- Compare experiments and runs
- View metrics over time
- Download artifacts
- Register models
- Track data lineage

### Airflow UI
```bash
# Access at: http://localhost:8080
```

**Features**:
- Monitor DAG runs
- View task logs
- Trigger manual runs
- Configure schedules
- Check task dependencies

---

## ğŸ“ˆ Model Performance

### Current Best Model
- **Algorithm**: Random Forest Classifier
- **Accuracy**: 0.776
- **Precision**: 0.767
- **Recall**: 0.763
- **F1-Score**: 0.776
- **ROC-AUC**: 0.89

---

## ğŸ¤ Contributing

We welcome contributions! Please follow these guidelines:

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Update documentation
6. Submit a pull request

### Code Style
- Follow PEP 8
- Use type hints
- Add docstrings
- Write unit tests

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ‘¥ Authors

- **Madhura Edirisooriya** - Initial work - [GitHub](https://github.com/madhurab00)

---

## ğŸ™ Acknowledgments

- Isuru Alagiyawanna(Machine Learning Zuu)
- Apache Airflow community
- MLflow contributors
- PySpark documentation
- Scikit-learn team

---

## ğŸ“š Additional Resources
- [Zuu Crew.ai](https://www.zuucrew.ai)
- [MLflow Documentation](https://mlflow.org/docs/latest/index.html)
- [Apache Airflow Documentation](https://airflow.apache.org/docs/)
- [PySpark Documentation](https://spark.apache.org/docs/latest/api/python/)
- [Scikit-learn User Guide](https://scikit-learn.org/stable/user_guide.html)

---

## ğŸ”— Related Projects

- [MLOps Best Practices](https://github.com/topics/mlops)
- [Production ML Systems](https://github.com/topics/production-ml)
- [Data Pipeline Examples](https://github.com/topics/data-pipeline)

---

**â­ If you find this project helpful, please consider giving it a star!**
